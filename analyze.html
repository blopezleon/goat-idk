<!DOCTYPE html>
<html lang="en">
<head>
   <meta charset="UTF-8">
   <meta name="viewport" content="width=device-width, initial-scale=1.0">
   <title>Speech Analysis - AI Speech Therapist</title>
   <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap" rel="stylesheet">
   <style>
       body {
           font-family: 'Poppins', sans-serif;
           margin: 0;
           padding: 0;
           background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
           min-height: 100vh;
       }


       .container {
           max-width: 800px;
           margin: 0 auto;
           padding: 2rem;
       }


       .analysis-section {
           background: white;
           padding: 2rem;
           border-radius: 10px;
           box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
           margin-bottom: 2rem;
       }


       .record-button {
           display: inline-block;
           padding: 1rem 2rem;
           background-color: #3498db;
           color: white;
           border: none;
           border-radius: 30px;
           font-weight: 500;
           cursor: pointer;
           transition: all 0.3s ease;
       }


       .record-button:hover {
           background-color: #2980b9;
       }


       .record-button.recording {
           background-color: #e74c3c;
       }


       #transcription, #feedback {
           margin-top: 1rem;
           padding: 1rem;
           background: #f8f9fa;
           border-radius: 5px;
           min-height: 100px;
       }


       .status {
           color: #7f8c8d;
           margin-top: 0.5rem;
       }


       .button-group {
           display: flex;
           gap: 1rem;
           margin-bottom: 1rem;
       }


       .play-button {
           display: inline-block;
           padding: 1rem 2rem;
           background-color: #27ae60;
           color: white;
           border: none;
           border-radius: 30px;
           font-weight: 500;
           cursor: pointer;
           transition: all 0.3s ease;
       }


       .play-button:hover {
           background-color: #219a52;
       }


       .play-button:disabled {
           background-color: #bdc3c7;
           cursor: not-allowed;
       }


       .audio-player {
           margin-top: 1rem;
           width: 100%;
       }
   </style>
</head>
<body>
   <div class="container">
       <div class="analysis-section">
           <h2>Speech Analysis</h2>
           <div class="button-group">
               <button id="recordButton" class="record-button">Start Recording</button>
               <button id="playButton" class="play-button" disabled>Play Recording</button>
           </div>
           <p class="status" id="status">Click to start recording</p>
          
           <audio id="audioPlayback" class="audio-player" controls style="display: none;"></audio>
          
           <h3>Your Speech</h3>
           <div id="transcription">Transcription will appear here...</div>
          
           <h3>AI Feedback</h3>
           <div id="feedback">Feedback will appear here...</div>
       </div>
   </div>


   <script>
       let mediaRecorder;
       let audioChunks = [];
      
       const recordButton = document.getElementById('recordButton');
       const playButton = document.getElementById('playButton');
       const status = document.getElementById('status');
       const audioPlayback = document.getElementById('audioPlayback');
       const transcriptionDiv = document.getElementById('transcription');
       const feedbackDiv = document.getElementById('feedback');


       recordButton.addEventListener('click', async () => {
           if (mediaRecorder && mediaRecorder.state === 'recording') {
               // Stop recording
               mediaRecorder.stop();
               recordButton.textContent = 'Start Recording';
               recordButton.classList.remove('recording');
               status.textContent = 'Processing...';
           } else {
               // Start recording
               try {
                   const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                   audioChunks = [];
                   
                   // Use a supported MIME type
                   const mimeType = 'audio/webm';
                   mediaRecorder = new MediaRecorder(stream, {
                       mimeType: mimeType,
                       audioBitsPerSecond: 16000
                   });
                   
                   mediaRecorder.ondataavailable = (event) => {
                       if (event.data.size > 0) {
                           audioChunks.push(event.data);
                       }
                   };

                   mediaRecorder.onstop = async () => {
                       const audioBlob = new Blob(audioChunks, { type: mimeType });
                       
                       // Show audio playback
                       const audioUrl = URL.createObjectURL(audioBlob);
                       audioPlayback.src = audioUrl;
                       audioPlayback.style.display = 'block';
                       playButton.disabled = false;

                       // Send to server
                       const reader = new FileReader();
                       reader.onloadend = async () => {
                           try {
                               const response = await fetch('http://localhost:3000/api/analyze-speech', {
                                   method: 'POST',
                                   headers: { 'Content-Type': 'application/json' },
                                   body: JSON.stringify({
                                       audio: reader.result,
                                       referenceText: "This is a test."
                                   })
                               });

                               if (!response.ok) {
                                   throw new Error(`Server error: ${response.status}`);
                               }

                               const results = await response.json();
                               transcriptionDiv.textContent = results.transcription || 'No transcription';
                               feedbackDiv.textContent = results.feedback || 'No feedback';
                               status.textContent = 'Analysis complete';

                           } catch (error) {
                               console.error('Error:', error);
                               status.textContent = `Error: ${error.message}`;
                           }
                       };

                       reader.readAsDataURL(audioBlob);
                   };

                   mediaRecorder.start();
                   recordButton.textContent = 'Stop Recording';
                   recordButton.classList.add('recording');
                   status.textContent = 'Recording...';

               } catch (error) {
                   console.error('Error:', error);
                   status.textContent = `Error: ${error.message}`;
               }
           }
       });


       // Audio playback controls
       playButton.addEventListener('click', () => {
           if (audioPlayback.paused) {
               audioPlayback.play();
               playButton.textContent = 'Pause';
           } else {
               audioPlayback.pause();
               playButton.textContent = 'Play Recording';
           }
       });


       audioPlayback.addEventListener('ended', () => {
           playButton.textContent = 'Play Recording';
       });


       audioPlayback.addEventListener('pause', () => {
           playButton.textContent = 'Play Recording';
       });


       audioPlayback.addEventListener('play', () => {
           playButton.textContent = 'Pause';
       });
   </script>

   <script>
       // Helper function to write strings to DataView
       function writeString(view, offset, string) {
           for (let i = 0; i < string.length; i++) {
               view.setUint8(offset + i, string.charCodeAt(i));
           }
       }
   </script>
</body>
</html>
